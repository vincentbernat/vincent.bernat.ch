---
title: "IntÃ©gration d'un service en Go avec systemd: activation par socket"
description: |
  En utilisant l'activation par socket de systemd, il est possible d'effectuer
  des dÃ©ploiements sans impact d'un service Go en quelques lignes de code.
uuid: 5463b0ca-6eb9-48e8-a36e-ed8a829fdb8d
tags:
  - programming-go
---

Dans un [article prÃ©cÃ©dent][previous post], j'ai soulignÃ© certaines
fonctionnalitÃ©s utiles de *systemd* pour Ã©crire un service en Go,
notamment pour indiquer la *disponibilitÃ©* et prouver la
*vivacitÃ©*. Un autre point intÃ©ressant est l'**activation par
socket**[^traduction]Â : *systemd* Ã©coute pour le compte de
l'application et, lors de la premiÃ¨re requÃªte, dÃ©marre le service avec
une copie de la socket en Ã©coute. Lennart Poettering [dÃ©taille dans un
article][details it in a blog post]Â :

> Si un service meurt, sa socket d'Ã©coute reste en place, sans perdre
> un seul message. AprÃ¨s un redÃ©marrage du service suite Ã  une panne,
> il peut continuer lÃ  oÃ¹ il s'est arrÃªtÃ©. Si un service est mis Ã 
> niveau, nous pouvons redÃ©marrer le service tout en conservant ses
> sockets, assurant ainsi que le service est continuellement
> disponible. Aucune connexion n'est perdue pendant la mise Ã  niveau.

[^traduction]: La traduction de *socket* en franÃ§ais n'est pas
    Ã©vidente. Quand le contexte est suffisamment clair, je m'amuse
    parfois Ã  dire Â«Â chaussetteÂ Â» ğŸ§¦ car "socket" est souvent Ã©crit
    `sock` dans les programmes. On pourrait le traduire par Â«Â prise
    rÃ©seauÂ Â», idÃ©al pour rendre perplexe la plupart des lecteurs.

Il s'agit d'une solution pour obtenir un **dÃ©ploiement sans impact**
d'une application. Un autre avantage est la possibilitÃ© d'exÃ©cuter un
dÃ©mon avec moins de privilÃ¨ges : perdre des droits est une tÃ¢che ardue
avec Go[^system].

[^system]: De nombreuses caractÃ©ristiques d'un processus sous Linux
    sont attachÃ©es aux fils d'exÃ©cution. L'environnement d'exÃ©cution
    de Go les gÃ¨re de maniÃ¨re transparente pour l'utilisateur. Jusqu'Ã 
    [rÃ©cemment][recently], cela rendait certaines fonctionnalitÃ©s,
    comme `setuid()` ou `setns()`, inutilisables.

[TOC]

# La base

Reprenons notre sympathique serveur de pages 404 :

    ::go
    package main
    
    import (
        "log"
        "net"
        "net/http"
    )
    
    func main() {
        listener, err := net.Listen("tcp", ":8081")
        if err != nil {
            log.Panicf("cannot listen: %s", err)
        }
        http.Serve(listener, nil)
    }

Voici la version utilisant l'activation par socket, Ã  l'aide de
[go-systemd][]Â :

    ::go hl_lines="11 19"
    package main
    
    import (
        "log"
        "net/http"
    
        "github.com/coreos/go-systemd/activation"
    )
    
    func main() {
        listeners, err := activation.Listeners(true) // â¶
        if err != nil {
            log.Panicf("cannot retrieve listeners: %s", err)
        }
        if len(listeners) != 1 {
            log.Panicf("unexpected number of socket activation (%d != 1)",
                len(listeners))
        }
        http.Serve(listeners[0], nil) // â·
    }

En â¶, nous rÃ©cupÃ©rons les sockets en Ã©coute fournies par *systemd*. En
â·, nous utilisons la premiÃ¨re d'entre elles pour servir les requÃªtes
HTTP. Testons le rÃ©sultat avec `systemd-socket-activate`[^old] :

    ::console
    $ go build 404.go
    $ systemd-socket-activate -l 8000 ./404
    Listening on [::]:8000 as 3.

[^old]: Avec d'anciennes versions de *systemd* (avant 230), la
    commande peut s'appeler `/lib/systemd/systemd-activate`.

Dans un autre terminal, effectuons quelques requÃªtesÂ :

    ::console
    $ curl '[::1]':8000
    404 page not found
    $ curl '[::1]':8000
    404 page not found

Deux fichiers sont nÃ©cessaires pour complÃ©ter l'intÃ©gration avec
*systemd* :

 - un fichier `.socket` dÃ©crivant la socket,
 - un fichier `.service` dÃ©crivant le service associÃ©.

Voici le contenu du fichier `404.socket` :

    ::ini
    [Socket]
    ListenStream = 8000
    BindIPv6Only = both
    
    [Install]
    WantedBy = sockets.target

La page de manuel [`systemd.socket(5)`][systemd.socket] dÃ©crit les
options disponibles. `BindIPv6Only = both` est explicitement utilisÃ©
car sa valeur par dÃ©faut dÃ©pend de la distribution. Voici ensuite le
contenu du fichier `404.service` :

    ::ini
    [Unit]
    Description = 404 micro-service
    
    [Service]
    ExecStart = /usr/bin/404

*systemd* sait que les deux fichiers sont liÃ©s car ils partagent un
mÃªme prÃ©fixe. Placez les dans `/etc/systemd/system` et exÃ©cutez
`systemctl daemon-reload` et `systemctl start 404.â€‹socket`. Votre
service est dÃ©sormais prÃªt Ã  accepter des connexions !

# Gestion des connexions existantes

Notre serveur de pages 404 a un dÃ©faut majeur : les connexions
existantes sont sauvagement tuÃ©es lorsque le dÃ©mon est arrÃªtÃ© ou
redÃ©marrÃ©. Corrigeons cela !

## Attendre quelques secondes les connexions existantes

Nous pouvons inclure une courte pÃ©riode de tolÃ©rance pour terminer les
connexions en cours. Ã€ l'issue de celles-ci, les connexions restantes
sont tuÃ©es :

    ::go hl_lines="15 22"
    // Ã€ la rÃ©ception du signal, ferme en douceur le serveur et
    // attend 5 secondes la fin des connexions en cours.
    done := make(chan struct{})
    quit := make(chan os.Signal, 1)
    server := &http.Server{}
    signal.Notify(quit, syscall.SIGINT, syscall.SIGTERM)
    
    go func() {
        <-quit
        log.Println("server is shutting down")
        ctx, cancel := context.WithTimeout(context.Background(),
            5*time.Second)
        defer cancel()
        server.SetKeepAlivesEnabled(false)
        if err := server.Shutdown(ctx); err != nil {
            log.Panicf("cannot gracefully shut down the server: %s", err)
        }
        close(done)
    }()

    // Accepte de nouvelles connexions.
    server.Serve(listeners[0])
    
    // Attend la fin des connexions en cours avant de sortir.
    <-done

Ã€ la rÃ©ception du signal de terminaison, la goroutine reprend et
[planifie l'arrÃªt du service][schedule a shutdown of the service] :

> `Shutdown()` ferme en douceur le serveur sans interrompre les
> connexions actives. `Shutdown()` fonctionne en fermant d'abord les
> sockets en Ã©coute puis en fermant toutes les connexions inactives et
> enfin en attendant indÃ©finiment que les connexions retombent au
> repos et s'arrÃªtent.

Durant le redÃ©marrage, les nouvelles connexions ne sont pas
acceptÃ©es : elles restent dans la file d'attente associÃ©e Ã  la
socket. La taille de celle-ci est limitÃ©e et peut Ãªtre configurÃ©e avec
la directive `Backlog`. Sa valeur par dÃ©faut est 128. Vous pouvez
conserver cette valeur mÃªme si votre service s'attend Ã  recevoir de
nombreuses connexions par seconde. Lorsque cette valeur est dÃ©passÃ©e,
les connexions entrantes sont ignorÃ©es. Le client rÃ©essaye
automatiquement de se connecter. Sous Linux, par dÃ©faut, un client
tente 5 fois (`tcp_syn_retries`) en 3 minutes environ. C'est un bon
moyen d'Ã©viter l'effet de troupeau qui se manifesterait au redÃ©marrage
si la taille de la file d'attente Ã©tait augmentÃ©e Ã  une valeur plus
Ã©levÃ©e.

## Attendre plus longtemps les connexions existantes

Si vous voulez attendre la fin des connexions existantes pendant une
longue pÃ©riode, vous avez besoin d'une approche alternative pour
Ã©viter d'ignorer les nouvelles connexions pendant plusieurs
minutes. Il y a une astuce trÃ¨s simple : **demander Ã  *systemd* de ne
tuer aucun processus**. Avec `KillMode = none`, seule la commande
d'arrÃªt est exÃ©cutÃ©e et tous les processus existants ne sont pas
perturbÃ©s :

    ::ini hl_lines="6 7"
    [Unit]
    Description = slow 404 micro-service
    
    [Service]
    ExecStart = /usr/bin/404
    ExecStop  = /bin/kill $MAINPID
    KillMode  = none

Si vous redÃ©marrez le service, le processus en cours prend le temps
nÃ©cessaire pour s'arrÃªter et *systemd* lance immÃ©diatement une
nouvelle instance, prÃªte Ã  rÃ©pondre aux requÃªtes avec sa propre copie
de la socket d'Ã©coute. Toutefois, nous perdons la capacitÃ© d'attendre
que le service s'arrÃªte complÃ¨tement, soit par lui-mÃªme, soit de force
aprÃ¨s un temps limite avec `SIGKILL`.

## Attendre plus longtemps les connexions existantes (alternative)

Une alternative Ã  la solution prÃ©cÃ©dente consiste Ã  **faire croire Ã 
*systemd* que le service est mort**ï¸ pendant le redÃ©marrage.

    ::go hl_lines="26 27"
    done := make(chan struct{})
    quit := make(chan os.Signal, 1)
    server := &http.Server{}
    signal.Notify(quit,
        // redÃ©marrage:
        syscall.SIGHUP,
        // arrÃªt:
        syscall.SIGINT, syscall.SIGTERM)
    go func() {
        sig := <-quit
        switch sig {
        case syscall.SIGINT, syscall.SIGTERM:
            // ArrÃªt avec limite de temps.
            log.Println("server is shutting down")
            ctx, cancel := context.WithTimeout(context.Background(),
                15*time.Second)
            defer cancel()
            server.SetKeepAlivesEnabled(false)
            if err := server.Shutdown(ctx); err != nil {
                log.Panicf("cannot gracefully shut down the server: %s", err)
            }
        case syscall.SIGHUP: // â¶
            // ExÃ©cute un processus Ã©phÃ©mÃ¨re et demande Ã  systemd de
            // le suivre au lieu de nous.
            log.Println("server is reloading")
            pid := detachedSleep()
            daemon.SdNotify(false, fmt.Sprintf("MAINPID=%d", pid))
            time.Sleep(time.Second)

            // Attend sans limite de temps la fin des connexions en cours.
            server.SetKeepAlivesEnabled(false)
            if err := server.Shutdown(context.Background()); err != nil {
                log.Panicf("cannot gracefully shut down the server: %s", err)
            }
        }
        close(done)
    }()

    // Sert lentement les requÃªtes.
    server.Handler = http.HandlerFunc(
        func(w http.ResponseWriter, r *http.Request) {
            time.Sleep(10 * time.Second)
            http.Error(w, "404 not found", http.StatusNotFound)
        })
    server.Serve(listeners[0])

    // Attend que toutes les connexions se terminent.
    <-done
    log.Println("server terminated")

La principale diffÃ©rence est le traitement du signal `SIGHUP` en â¶ :
un processus leurre de courte durÃ©e est exÃ©cutÃ© et *systemd* est
invitÃ© Ã  le suivre. Quand il meurt, *systemd* lancera une nouvelle
instance du service. Cette mÃ©thode nÃ©cessite quelques bricolages :
*systemd* a besoin que le leurre soit son fils mais Go [ne peut pas
facilement se mettre en arriÃ¨re plan][cannot daemonize] seul. Par
consÃ©quent, nous utilisons un court script Python inclu dans la
fonction `detachedSleep()`[^python]Â :

    ::go
    // detachedSleep exÃ©cute un processus dormant une seconde
    // en arriÃ¨re plan et retourne son PID.
    func detachedSleep() uint64 {
        py := `
    import os
    import time
    
    pid = os.fork()
    if pid == 0:
        for fd in {0, 1, 2}:
            os.close(fd)
        time.sleep(1)
    else:
        print(pid)
    `
        cmd := exec.Command("/usr/bin/python3", "-c", py)
        out, err := cmd.Output()
        if err != nil {
            log.Panicf("cannot execute sleep command: %s", err)
        }
        pid, err := strconv.ParseUint(strings.TrimSpace(string(out)), 10, 64)
        if err != nil {
            log.Panicf("cannot parse PID of sleep command: %s", err)
        }
        return pid
    }

[^python]: Python est un bon candidat : il est sans doute disponible
    sur le systÃ¨me, il est d'assez bas niveau pour implÃ©menter
    facilement la fonctionnalitÃ© et, en tant que langage interprÃ©tÃ©,
    il ne nÃ©cessite pas d'Ã©tape de compilation.
    
    Il n'y a pas besoin d'appeler `fork()`
    deux fois car il faut uniquement dÃ©tacher le leurre du processus
    courant. Cela simplifie sensiblement le code Python.

Pendant le rechargement, il peut y avoir une courte pÃ©riode pendant
laquelle le nouveau et l'ancien processus acceptent les requÃªtes
entrantes. Si vous ne le souhaitez pas, vous pouvez dÃ©placer la
crÃ©ation du processus leurre en dehors de la goroutine, aprÃ¨s
`server.Serve()` ou implÃ©menter un mÃ©canisme de synchronisation. Il y
a aussi un Ã©ventuel problÃ¨me de concurrence lorsque nous disons Ã 
*systemd* de suivre un autre PID (voir [PR #7816][]).

Le fichier `404.service` doit Ãªtre mis Ã  jour :

    ::ini
    [Unit]
    Description = slow 404 micro-service
    
    [Service]
    ExecStart    = /usr/bin/404
    ExecReload   = /bin/kill -HUP $MAINPID
    Restart      = always
    NotifyAccess = main
    KillMode     = process

Chacune des directives supplÃ©mentaires a son importance :

 - `ExecReload` indique comment recharger le processus (avec `SIGHUP`).
 - `Restart` indique de redÃ©marrer le processus s'il s'arrÃªte de
   maniÃ¨re Â« inattendue Â», notamment lors du rechargement[^socket].
 - `NotifyAccess` prÃ©cise quels sont les processus autorisÃ©s Ã  envoyer
   des notifications comme le changement de PID.
 - `KillMode` indique de ne tuer que le processus identifiÃ© comme
   principal. Les autres sont laissÃ©s tranquilles.

[^socket]: Cette directive n'est pas essentielle car le processus
    serait aussi redÃ©marrÃ© via l'activation de la socket.

## DÃ©ploiement sans impactÂ ?

Le dÃ©ploiement sans impact est une entreprise difficile sur Linux. Par
exemple, [HAProxy][] a eu une [longue][hack1] [liste][hack2] de
[tentatives][hack3] jusqu'Ã  ce qu'une [solution appropriÃ©e, mais
complexe][properâ€”and complexâ€”solution], soit implÃ©mentÃ©e dans
*HAproxy* 1.8. Comment se dÃ©brouille-t-on avec notre simple mise en
Å“uvre ?

Du point de vue du noyau, il y a **une seule socket** avec une **file
d'attente unique**. Cette socket est associÃ©e Ã  plusieurs descripteurs
de fichiers : un dans *systemd* et un dans le processus en cours. La
chaussette reste en vie tant qu'il y a au moins un descripteur de
fichier. Une connexion entrante est placÃ©e par le noyau dans la file
d'attente et peut Ãªtre traitÃ©e Ã  partir de n'importe quel descripteur
avec l'appel systÃ¨me `accept()`. Par consÃ©quent, cette approche permet
de rÃ©aliser un dÃ©ploiement sans impact : aucune connexion entrante
n'est rejetÃ©e.

En revanche, *HAProxy* utilisait plusieurs sockets diffÃ©rentes pour
Ã©couter sur les mÃªmes adresses, grÃ¢ce Ã  l'option
[`SO_REUSEPORT`][SO_REUSEPORT][^why]. Chaque socket a sa propre file
d'attente et le noyau rÃ©partit les connexions entrantes entre chacune
d'elles. Lorsqu'une socket se ferme, le contenu de sa file d'attente
est perdu. Si une connexion entrante se trouvait ici, elle reÃ§oit une
rÃ©initialisation. Une modification Ã©lÃ©gante pour Linux afin de
[signaler qu'une socket ne devrait plus recevoir de nouvelles
connexions][signal a socket should not receive new connections] a Ã©tÃ©
rejetÃ©e. *HAProxy* 1.8 recycle dÃ©sormais les sockets existantes vers
les nouveaux processus par le biais d'une socket Unix.

[^why]: Cette approche est plus pratique lors d'un rechargement car il
    n'y a pas Ã  dÃ©terminer quelles sockets rÃ©utiliser et lesquelles
    crÃ©er Ã  partir de zÃ©ro. De plus, lorsque plusieurs processus ont
    besoin d'accepter des connexions, l'utilisation de plusieurs
    sockets est plus performante car les diffÃ©rents processus ne se
    disputeront pas sur un verrou partagÃ© pour accepter des
    connexions.

J'espÃ¨re que ce billet et le [prÃ©cÃ©dent][previous post] montrent
combien *systemd* est un compagnon apprÃ©ciable pour un service en Go :
*disponibilitÃ©*, *vivacitÃ©* et *activation par socket* sont quelques
unes des fonctionnalitÃ©s utiles pour construire une application plus
fiable.

# Annexe: leurre Ã©crit en Go

!!! "Mise Ã  jour (03.2018)" Sur [/r/golang][], on m'a fait remarquer
que, dans la version oÃ¹ *systemd* suit un leurre, le script Python
peut Ãªtre remplacÃ© par une invocation de l'exÃ©cutable principal qui se
base sur un changement d'environnement pour prendre le rÃ´le du
leurre. Voici le code remplaÃ§ant la fonction `detachedSleep()`
function :

    ::go
    func init() {
        // Au plus tÃ´t, vÃ©rifie si on doit jouer le rÃ´le
        // du leurre.
        state := os.Getenv("__SLEEPY")
        os.Unsetenv("__SLEEPY")
        switch state {
        case "1":
            // PremiÃ¨re Ã©tape, se rÃ©exÃ©cuter
            execPath := self()
            child, err := os.StartProcess(
                execPath,
                []string{execPath},
                &os.ProcAttr{
                    Env: append(os.Environ(), "__SLEEPY=2"),
                })
            if err != nil {
                log.Panicf("cannot execute sleep command: %s", err)
            }
    
            // Publie le PID du fils et sort.
            fmt.Printf("%d", child.Pid)
            os.Exit(0)
        case "2":
            // Dort et sort.
            time.Sleep(time.Second)
            os.Exit(0)
        }
    }
    
    // self retourne le chemin absolu vers nous-mÃªme. Cela repose sur
    // /proc/self/exe qui peut Ãªtre un lien symbolique vers un fichier
    // supprimÃ© (durant une mise Ã  jour par exemple).
    func self() string {
        execPath, err := os.Readlink("/proc/self/exe")
        if err != nil {
            log.Panicf("cannot get self path: %s", err)
        }
        execPath = strings.TrimSuffix(execPath, " (deleted)")
        return execpath
    }
    
    // detachedSleep dÃ©tache un processus qui dort une seconde et retourne
    // son PID.
    func detachedSleep() uint64 {
        cmd := exec.Command(self())
        cmd.Env = append(os.Environ(), "__SLEEPY=1")
        out, err := cmd.Output()
        if err != nil {
            log.Panicf("cannot execute sleep command: %s", err)
        }
        pid, err := strconv.ParseUint(strings.TrimSpace(string(out)), 10, 64)
        if err != nil {
            log.Panicf("cannot parse PID of sleep command: %s", err)
        }
        return pid
    }

# Annexe : nommage des sockets

Pour un service donnÃ©, *systemd* peut fournir plusieurs sockets. Pour
les diffÃ©rencier, il est possible de les nommer. Par exemple,
supposons que nous voulions aussi retourner des codes d'erreur 403
depuis le mÃªme service mais sur un port diffÃ©rent. Nous ajoutons une
dÃ©finition de socket supplÃ©mentaire, `403.socket`, liÃ©e Ã  la tÃ¢che
`404.service` :

    ::ini hl_lines="4"
    [Socket]
    ListenStream = 8001
    BindIPv6Only = both
    Service      = 404.service
    
    [Install]
    WantedBy=sockets.target

Ã€ moins de le spÃ©cifier explicitement avec la directive
`FileDescriptorName`, le nom de la socket est le nom de l'unitÃ© :
`403.socket`. *go-systemd* fournit la fonction `ListenersWithName()`
pour rÃ©cupÃ©rer une correspondance entre les noms et les sockets :

    ::go hl_lines="24"
    package main
    
    import (
        "log"
        "net/http"
        "sync"
    
        "github.com/coreos/go-systemd/activation"
    )
    
    func main() {
        var wg sync.WaitGroup

        // Associe un nom de socket Ã  une fonction de gestion.
        handlers := map[string]http.HandlerFunc{
            "404.socket": http.NotFound,
            "403.socket": func(w http.ResponseWriter, r *http.Request) {
                http.Error(w, "403 forbidden",
                    http.StatusForbidden)
            },
        }
    
        // RÃ©cupÃ¨re les sockets en Ã©coute.
        listeners, err := activation.ListenersWithNames(true)
        if err != nil {
            log.Panicf("cannot retrieve listeners: %s", err)
        }
    
        // Pour chaque socket, invoque une goroutine en utilisant
        // la fonction de gestion adÃ©quate.
        for name := range listeners {
            for idx := range listeners[name] {
                wg.Add(1)
                go func(name string, idx int) {
                    defer wg.Done()
                    http.Serve(
                        listeners[name][idx],
                        handlers[name])
                }(name, idx)
            }
        }
    
        // Attend que toutes les goroutines terminent.
        wg.Wait()
    }

Compilons le service et lanÃ§ons le via `systemd-socket-activate`:

    ::console
    $ go build 404.go
    $ systemd-socket-activate -l 8000 -l 8001 \
                              --fdname=404.socket:403.socket \
                              ./404
    Listening on [::]:8000 as 3.
    Listening on [::]:8001 as 4.

Dans une autre console, nous pouvons tester une requÃªte sur chacune
des deux adresses :

    ::console
    $ curl '[::1]':8000
    404 page not found
    $ curl '[::1]':8001
    403 forbidden

[previous post]: [[fr/blog/2017-systemd-golang.html]] "IntÃ©gration d'un service en Go avec systemd: disponibilitÃ© et vivacitÃ©"
[details it in a blog post]: http://0pointer.de/blog/projects/socket-activation.html "systemd for Developers I: socket activation"
[go-systemd]: https://github.com/coreos/go-systemd/ "Go bindings for systemd"
[systemd.socket]: https://manpages.debian.org/stretch/systemd/systemd.socket.5.en.html "systemd.socket(5) manual page"
[schedule a shutdown of the service]: https://godoc.org/net/http#Server.Shutdown "godoc: net/http Server Shutdown"
[endless]: https://github.com/fvbock/endless "Zero downtime restarts for go servers"
[yet]: https://github.com/coreos/go-systemd/pull/251 "activation: add two functions to provide listeners with names"
[HAProxy]: https://www.haproxy.org/ "HAProxy: The Reliable, High Performance TCP/HTTP Load Balancer"
[hack1]: https://engineeringblog.yelp.com/2015/04/true-zero-downtime-haproxy-reloads.html "True Zero Downtime HAProxy Reload"
[hack2]: https://medium.com/@Drew_Stokes/actual-zero-downtime-with-haproxy-18318578fde6 "Actual Zero-Downtime with HAProxy"
[hack3]: https://githubengineering.com/glb-part-2-haproxy-zero-downtime-zero-delay-reloads-with-multibinder/ "GLB part 2: HAProxy zero-downtime, zero-delay reloads with multibinder"
[properâ€”and complexâ€”solution]: https://www.haproxy.com/blog/truly-seamless-reloads-with-haproxy-no-more-hacks/ "Truly Seamless Reloads with HAProxy â€“ No More Hacks!"
[SO_REUSEPORT]: https://manpages.debian.org/stretch/manpages/socket.7.en.html "socket - Linux socket interface"
[signal a socket should not receive new connections]: https://www.mail-archive.com/netdev@vger.kernel.org/msg91809.html "Re: [PATCH 1/1] net: Add SO_REUSEPORT_LISTEN_OFF socket option as drain mode"
[dup2]: https://manpages.debian.org/stretch/manpages-fr-dev/dup2.2.fr.html "dup, dup2, dup3 - Dupliquer un descripteur de fichier"
[PR #7816]: https://github.com/systemd/systemd/pull/7816 "Make MAINPID= and PIDFile= handling more restrictive"
[cannot daemonize]: https://github.com/golang/go/issues/227 "runtime: support for daemonize"
[recently]: https://golang.org/doc/go1.10#runtime "Go 1.10 Release Notes: Runtime"
[/r/golang]: https://www.reddit.com/r/golang/comments/85sm59/integration_of_a_go_service_with_systemd_socket/ "/r/golang: Integration of a Go service with systemd: socket activation"
